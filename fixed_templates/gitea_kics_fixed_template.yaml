apiVersion: v1
kind: Secret
metadata:
  name: release-name-postgresql
  namespace: test-ns
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  postgres-password: eGNvb0Qyd2J4NQ==
  password: Z2l0ZWE=
---
apiVersion: v1
kind: Secret
metadata:
  name: release-name-gitea-inline-config
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  namespace: test-ns
type: Opaque
stringData:
  _generals_: ''
  cache: 'ADAPTER=memcache

    ENABLED=true

    HOST=release-name-memcached.default.svc.cluster.local:11211'
  database: 'DB_TYPE=postgres

    HOST=release-name-postgresql.default.svc.cluster.local:5432

    NAME=gitea

    PASSWD=gitea

    USER=gitea'
  metrics: ENABLED=false
  repository: ROOT=/data/git/gitea-repositories
  security: INSTALL_LOCK=true
  server: 'APP_DATA_PATH=/data

    DOMAIN=git.example.com

    ENABLE_PPROF=false

    HTTP_PORT=3000

    PROTOCOL=http

    ROOT_URL=http://git.example.com

    SSH_DOMAIN=git.example.com

    SSH_LISTEN_PORT=22

    SSH_PORT=22'
---
apiVersion: v1
kind: Secret
metadata:
  name: release-name-gitea
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  namespace: test-ns
type: Opaque
stringData:
  config_environment.sh: "#!/usr/bin/env bash\nset -euo pipefail\n\nfunction env2ini::log()\
    \ {\n  printf \"${1}\\n\"\n}\n\nfunction env2ini::read_config_to_env() {\n  local\
    \ section=\"${1}\"\n  local line=\"${2}\"\n\n  if [[ -z \"${line}\" ]]; then\n\
    \    # skip empty line\n    return\n  fi\n  \n  # 'xargs echo -n' trims all leading/trailing\
    \ whitespaces and a trailing new line\n  local setting=\"$(awk -F '=' '{print\
    \ $1}' <<< \"${line}\" | xargs echo -n)\"\n\n  if [[ -z \"${setting}\" ]]; then\n\
    \    env2ini::log '  ! invalid setting'\n    exit 1\n  fi\n\n  local value=''\n\
    \  local regex=\"^${setting}(\\s*)=(\\s*)(.*)\"\n  if [[ $line =~ $regex ]]; then\n\
    \    value=\"${BASH_REMATCH[3]}\"\n  else\n    env2ini::log '  ! invalid setting'\n\
    \    exit 1\n  fi\n\n  env2ini::log \"    + '${setting}'\"\n\n  if [[ -z \"${section}\"\
    \ ]]; then\n    export \"ENV_TO_INI____${setting^^}=${value}\"               \
    \            # '^^' makes the variable content uppercase\n    return\n  fi\n\n\
    \  local masked_section=\"${section//./_0X2E_}\"                            #\
    \ '//' instructs to replace all matches\n  masked_section=\"${masked_section//-/_0X2D_}\"\
    \n\n  export \"ENV_TO_INI__${masked_section^^}__${setting^^}=${value}\"      \
    \  # '^^' makes the variable content uppercase\n}\n\nfunction env2ini::reload_preset_envs()\
    \ {\n  env2ini::log \"Reloading preset envs...\"\n\n  while read -r line; do\n\
    \    if [[ -z \"${line}\" ]]; then\n      # skip empty line\n      return\n  \
    \  fi\n\n    # 'xargs echo -n' trims all leading/trailing whitespaces and a trailing\
    \ new line\n    local setting=\"$(awk -F '=' '{print $1}' <<< \"${line}\" | xargs\
    \ echo -n)\"\n\n    if [[ -z \"${setting}\" ]]; then\n      env2ini::log '  !\
    \ invalid setting'\n      exit 1\n    fi\n\n    local value=''\n    local regex=\"\
    ^${setting}(\\s*)=(\\s*)(.*)\"\n    if [[ $line =~ $regex ]]; then\n      value=\"\
    ${BASH_REMATCH[3]}\"\n    else\n      env2ini::log '  ! invalid setting'\n   \
    \   exit 1\n    fi\n\n    env2ini::log \"  + '${setting}'\"\n\n    export \"${setting^^}=${value}\"\
    \                           # '^^' makes the variable content uppercase\n  done\
    \ < \"/tmp/existing-envs\"\n\n  rm /tmp/existing-envs\n}\n\n\nfunction env2ini::process_config_file()\
    \ {\n  local config_file=\"${1}\"\n  local section=\"$(basename \"${config_file}\"\
    )\"\n\n  if [[ $section == '_generals_' ]]; then\n    env2ini::log \"  [ini root]\"\
    \n    section=''\n  else\n    env2ini::log \"  ${section}\"\n  fi\n\n  while read\
    \ -r line; do\n    env2ini::read_config_to_env \"${section}\" \"${line}\"\n  done\
    \ < <(awk 1 \"${config_file}\")                             # Helm .toYaml trims\
    \ the trailing new line which breaks line processing; awk 1 ... adds it back while\
    \ reading\n}\n\nfunction env2ini::load_config_sources() {\n  local path=\"${1}\"\
    \n\n  if [[ -d \"${path}\" ]]; then\n    env2ini::log \"Processing $(basename\
    \ \"${path}\")...\"\n\n    while read -d '' configFile; do\n      env2ini::process_config_file\
    \ \"${configFile}\"\n    done < <(find \"${path}\" -type l -not -name '..data'\
    \ -print0)\n\n    env2ini::log \"\\n\"\n  fi\n}\n\nfunction env2ini::generate_initial_secrets()\
    \ {\n  # These environment variables will either be\n  #   - overwritten with\
    \ user defined values,\n  #   - initially used to set up Gitea\n  # Anyway, they\
    \ won't harm existing app.ini files\n\n  export ENV_TO_INI__SECURITY__INTERNAL_TOKEN=$(gitea\
    \ generate secret INTERNAL_TOKEN)\n  export ENV_TO_INI__SECURITY__SECRET_KEY=$(gitea\
    \ generate secret SECRET_KEY)\n  export ENV_TO_INI__OAUTH2__JWT_SECRET=$(gitea\
    \ generate secret JWT_SECRET)\n  export ENV_TO_INI__SERVER__LFS_JWT_SECRET=$(gitea\
    \ generate secret LFS_JWT_SECRET)\n\n  env2ini::log \"...Initial secrets generated\\\
    n\"\n}\n\nenv | (grep ENV_TO_INI || [[ $? == 1 ]]) > /tmp/existing-envs\n\n# MUST\
    \ BE CALLED BEFORE OTHER CONFIGURATION\nenv2ini::generate_initial_secrets\n\n\
    env2ini::load_config_sources '/env-to-ini-mounts/inlines/'\nenv2ini::load_config_sources\
    \ '/env-to-ini-mounts/additionals/'\n\n# load existing envs to override auto generated\
    \ envs\nenv2ini::reload_preset_envs\n\nenv2ini::log \"=== All configuration sources\
    \ loaded ===\\n\"\n\n# safety to prevent rewrite of secret keys if an app.ini\
    \ already exists\nif [ -f ${GITEA_APP_INI} ]; then\n  env2ini::log 'An app.ini\
    \ file already exists. To prevent overwriting secret keys, these settings are\
    \ dropped and remain unchanged:'\n  env2ini::log '  - security.INTERNAL_TOKEN'\n\
    \  env2ini::log '  - security.SECRET_KEY'\n  env2ini::log '  - oauth2.JWT_SECRET'\n\
    \  env2ini::log '  - server.LFS_JWT_SECRET'\n\n  unset ENV_TO_INI__SECURITY__INTERNAL_TOKEN\n\
    \  unset ENV_TO_INI__SECURITY__SECRET_KEY\n  unset ENV_TO_INI__OAUTH2__JWT_SECRET\n\
    \  unset ENV_TO_INI__SERVER__LFS_JWT_SECRET\nfi\n\nenvironment-to-ini -o $GITEA_APP_INI\
    \ -p ENV_TO_INI"
---
apiVersion: v1
kind: Secret
metadata:
  name: release-name-gitea-init
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  namespace: test-ns
type: Opaque
stringData:
  configure_gpg_environment.sh: '#!/usr/bin/env bash

    set -eu


    gpg --batch --import /raw/private.asc'
  init_directory_structure.sh: '#!/usr/bin/env bash


    set -euo pipefail


    set -x

    chown 1000:1000 /data

    mkdir -p /data/git/.ssh

    chmod -R 700 /data/git/.ssh

    [ ! -d /data/gitea/conf ] && mkdir -p /data/gitea/conf


    # prepare temp directory structure

    mkdir -p "${GITEA_TEMP}"

    chown 1000:1000 "${GITEA_TEMP}"

    chmod ug+rwx "${GITEA_TEMP}"'
  configure_gitea.sh: "#!/usr/bin/env bash\n\nset -euo pipefail\n\necho '==== BEGIN\
    \ GITEA CONFIGURATION ===='\n\n{ # try\n  gitea migrate\n} || { # catch\n  echo\
    \ \"Gitea migrate might fail due to database connection...This init-container\
    \ will try again in a few seconds\"\n  exit 1\n}\nfunction configure_admin_user()\
    \ {\n  local ACCOUNT_ID=$(gitea admin user list --admin | grep -e \"\\s\\+${GITEA_ADMIN_USERNAME}\\\
    s\\+\" | awk -F \" \" \"{printf \\$1}\")\n  if [[ -z \"${ACCOUNT_ID}\" ]]; then\n\
    \    echo \"No admin user '${GITEA_ADMIN_USERNAME}' found. Creating now...\"\n\
    \    gitea admin user create --admin --username \"${GITEA_ADMIN_USERNAME}\" --password\
    \ \"${GITEA_ADMIN_PASSWORD}\" --email \"gitea@local.domain\" --must-change-password=false\n\
    \    echo '...created.'\n  else\n    echo \"Admin account '${GITEA_ADMIN_USERNAME}'\
    \ already exist. Running update to sync password...\"\n    gitea admin user change-password\
    \ --username \"${GITEA_ADMIN_USERNAME}\" --password \"${GITEA_ADMIN_PASSWORD}\"\
    \n    echo '...password sync done.'\n  fi\n}\n\nconfigure_admin_user\n\nfunction\
    \ configure_ldap() {\n    echo 'no ldap configuration... skipping.'\n}\n\nconfigure_ldap\n\
    \nfunction configure_oauth() {\n    echo 'no oauth configuration... skipping.'\n\
    }\n\nconfigure_oauth\n\necho '==== END GITEA CONFIGURATION ===='"
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-memcached
  namespace: test-ns
  labels:
    app.kubernetes.io/name: memcached
    helm.sh/chart: memcached-6.3.14
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations: null
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
  - name: memcache
    port: 11211
    targetPort: memcache
    nodePort: null
  selector:
    app.kubernetes.io/name: memcached
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql-hl
  namespace: test-ns
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: tcp-postgresql
    port: 5432
    targetPort: tcp-postgresql
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-postgresql
  namespace: test-ns
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
  - name: tcp-postgresql
    port: 5432
    targetPort: tcp-postgresql
    nodePort: null
  selector:
    app.kubernetes.io/name: postgresql
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: primary
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-gitea-http
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  annotations: {}
  namespace: test-ns
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: http
    port: 3000
    targetPort: 3000
  selector:
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
---
apiVersion: v1
kind: Service
metadata:
  name: release-name-gitea-ssh
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  annotations: {}
  namespace: test-ns
spec:
  type: ClusterIP
  clusterIP: None
  ports:
  - name: ssh
    port: 22
    targetPort: 22
    protocol: TCP
  selector:
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-memcached
  namespace: test-ns
  labels:
    app.kubernetes.io/name: memcached
    helm.sh/chart: memcached-6.3.14
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
  annotations:
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: memcached
      app.kubernetes.io/instance: release-name
  replicas: 1
  strategy:
    rollingUpdate: {}
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: memcached
        helm.sh/chart: memcached-6.3.14
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
      annotations:
        container.apparmor.security.beta.kubernetes.io/memcached: runtime/default
    spec:
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: memcached
                  app.kubernetes.io/instance: release-name
              topologyKey: kubernetes.io/hostname
            weight: 1
        nodeAffinity: null
      securityContext:
        fsGroup: 1001
        runAsUser: 25000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: memcached
        image: docker.io/bitnami/memcached:1@sha256:0466889ea7a59c3e5075122e0dfac5f9fd4b740dbc54d7ca3355e661704b7df3
        imagePullPolicy: Always
        securityContext:
          runAsNonRoot: true
          runAsUser: 25000
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        env:
        - name: BITNAMI_DEBUG
          value: 'false'
        - name: MEMCACHED_PORT_NUMBER
          value: '11211'
        ports:
        - name: memcache
          containerPort: 11211
        livenessProbe:
          failureThreshold: 6
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
          tcpSocket:
            port: memcache
        readinessProbe:
          failureThreshold: 6
          initialDelaySeconds: 5
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 3
          tcpSocket:
            port: memcache
        resources:
          limits:
            cpu: 250m
            memory: 256Mi
          requests:
            cpu: 250m
            memory: 256Mi
        volumeMounts:
        - name: tmp
          mountPath: /tmp
      volumes:
      - name: tmp
        emptyDir: {}
      automountServiceAccountToken: false
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-postgresql
  namespace: test-ns
  labels:
    app.kubernetes.io/name: postgresql
    helm.sh/chart: postgresql-12.4.1
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: primary
  annotations:
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
spec:
  replicas: 1
  serviceName: release-name-postgresql-hl
  updateStrategy:
    rollingUpdate: {}
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: postgresql
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: primary
  template:
    metadata:
      name: release-name-postgresql
      labels:
        app.kubernetes.io/name: postgresql
        helm.sh/chart: postgresql-12.4.1
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: primary
      annotations:
        container.apparmor.security.beta.kubernetes.io/postgresql: runtime/default
    spec:
      affinity:
        podAffinity: null
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  app.kubernetes.io/name: postgresql
                  app.kubernetes.io/instance: release-name
                  app.kubernetes.io/component: primary
              topologyKey: kubernetes.io/hostname
            weight: 1
        nodeAffinity: null
      securityContext:
        fsGroup: 1001
        runAsUser: 25000
        seccompProfile:
          type: RuntimeDefault
      hostNetwork: false
      hostIPC: false
      containers:
      - name: postgresql
        image: docker.io/bitnami/postgresql:15.2.0-debian-11-r26
        imagePullPolicy: Always
        securityContext:
          runAsUser: 25000
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        env:
        - name: BITNAMI_DEBUG
          value: 'false'
        - name: POSTGRESQL_PORT_NUMBER
          value: '5432'
        - name: POSTGRESQL_VOLUME_DIR
          value: /bitnami/postgresql
        - name: PGDATA
          value: /bitnami/postgresql/data
        - name: POSTGRES_USER
          value: gitea
        - name: POSTGRES_DB
          value: gitea
        - name: POSTGRESQL_ENABLE_LDAP
          value: 'no'
        - name: POSTGRESQL_ENABLE_TLS
          value: 'no'
        - name: POSTGRESQL_LOG_HOSTNAME
          value: 'false'
        - name: POSTGRESQL_LOG_CONNECTIONS
          value: 'false'
        - name: POSTGRESQL_LOG_DISCONNECTIONS
          value: 'false'
        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG
          value: 'off'
        - name: POSTGRESQL_CLIENT_MIN_MESSAGES
          value: error
        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES
          value: pgaudit
        ports:
        - name: tcp-postgresql
          containerPort: 5432
        livenessProbe:
          failureThreshold: 6
          initialDelaySeconds: 30
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
          exec:
            command:
            - /bin/sh
            - -c
            - exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432
        readinessProbe:
          failureThreshold: 6
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
          exec:
            command:
            - /bin/sh
            - -c
            - -e
            - 'exec pg_isready -U "gitea" -d "dbname=gitea" -h 127.0.0.1 -p 5432

              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized
              ]

              '
        resources:
          limits:
            cpu: 250m
            memory: 256Mi
          requests:
            cpu: 250m
            memory: 256Mi
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
          readOnly: true
        - name: data
          mountPath: /bitnami/postgresql
          readOnly: true
        - name: secret-volume
          readOnly: true
          mountPath: /etc/secret-volume
        - name: secret-volume
          readOnly: true
          mountPath: /etc/secret-volume
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      - name: secret-volume
        secret:
          secretName: my-secret
      - name: secret-volume
        secret:
          secretName: my-secret
      automountServiceAccountToken: false
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources: {}
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-gitea
  annotations:
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  namespace: test-ns
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: gitea
      app.kubernetes.io/instance: release-name
  serviceName: release-name-postgresql-hl
  template:
    metadata:
      annotations:
        checksum/config: 16ee04567b54155f5ccc612683a37fffce14728cd3f5fc615330f310d8fca043
        container.apparmor.security.beta.kubernetes.io/gitea: runtime/default
        container.apparmor.security.beta.kubernetes.io/init-directories: runtime/default
        container.apparmor.security.beta.kubernetes.io/init-app-ini: runtime/default
        container.apparmor.security.beta.kubernetes.io/configure-gitea: runtime/default
      labels:
        helm.sh/chart: gitea-8.3.0
        app: gitea
        app.kubernetes.io/name: gitea
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: 1.19.3
        version: 1.19.3
        app.kubernetes.io/managed-by: Helm
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 25000
        seccompProfile:
          type: RuntimeDefault
      initContainers:
      - name: init-directories
        image: gitea/gitea:1.19.3
        imagePullPolicy: Always
        command:
        - /usr/sbin/init_directory_structure.sh
        env:
        - name: GITEA_APP_INI
          value: /data/gitea/conf/app.ini
        - name: GITEA_CUSTOM
          value: /data/gitea
        - name: GITEA_WORK_DIR
          value: /data
        - name: GITEA_TEMP
          value: /tmp/gitea
        volumeMounts:
        - name: init
          mountPath: /usr/sbin
          readOnly: true
        - name: temp
          mountPath: /tmp
          readOnly: true
        - name: data
          mountPath: /data
          readOnly: true
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 25000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
      - name: init-app-ini
        image: gitea/gitea:1.19.3
        imagePullPolicy: Always
        command:
        - /usr/sbin/config_environment.sh
        env:
        - name: GITEA_APP_INI
          value: /data/gitea/conf/app.ini
        - name: GITEA_CUSTOM
          value: /data/gitea
        - name: GITEA_WORK_DIR
          value: /data
        - name: GITEA_TEMP
          value: /tmp/gitea
        volumeMounts:
        - name: config
          mountPath: /usr/sbin
          readOnly: true
        - name: temp
          mountPath: /tmp
          readOnly: true
        - name: data
          mountPath: /data
          readOnly: true
        - name: inline-config-sources
          mountPath: /env-to-ini-mounts/inlines/
          readOnly: true
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 25000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
      - name: configure-gitea
        image: gitea/gitea:1.19.3
        command:
        - /usr/sbin/configure_gitea.sh
        imagePullPolicy: Always
        securityContext:
          runAsUser: 25000
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        env:
        - name: GITEA_APP_INI
          value: /data/gitea/conf/app.ini
        - name: GITEA_CUSTOM
          value: /data/gitea
        - name: GITEA_WORK_DIR
          value: /data
        - name: GITEA_TEMP
          value: /tmp/gitea
        - name: GITEA_ADMIN_USERNAME
          value: gitea_admin
        - name: GITEA_ADMIN_PASSWORD
          value: r8sA8CPHD9!bt6d
        volumeMounts:
        - name: init
          mountPath: /usr/sbin
          readOnly: true
        - name: temp
          mountPath: /tmp
          readOnly: true
        - name: data
          mountPath: /data
          readOnly: true
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 128Mi
      terminationGracePeriodSeconds: 60
      containers:
      - name: gitea
        image: gitea/gitea:1.19.3
        imagePullPolicy: Always
        env:
        - name: SSH_LISTEN_PORT
          value: '22'
        - name: SSH_PORT
          value: '22'
        - name: SSH_LOG_LEVEL
          value: INFO
        - name: GITEA_APP_INI
          value: /data/gitea/conf/app.ini
        - name: GITEA_CUSTOM
          value: /data/gitea
        - name: GITEA_WORK_DIR
          value: /data
        - name: GITEA_TEMP
          value: /tmp/gitea
        - name: TMPDIR
          value: /tmp/gitea
        ports:
        - name: ssh
          containerPort: 22
        - name: http
          containerPort: 3000
        livenessProbe:
          failureThreshold: 10
          initialDelaySeconds: 200
          periodSeconds: 10
          successThreshold: 1
          tcpSocket:
            port: http
          timeoutSeconds: 1
        readinessProbe:
          failureThreshold: 3
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          tcpSocket:
            port: http
          timeoutSeconds: 1
        resources:
          limits:
            cpu: 250m
            memory: 128Mi
          requests:
            cpu: 250m
            memory: 128Mi
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 25000
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
        volumeMounts:
        - name: temp
          mountPath: /tmp
        - name: data
          mountPath: /data
      volumes:
      - name: init
        secret:
          secretName: release-name-gitea-init
          defaultMode: 110
      - name: config
        secret:
          secretName: release-name-gitea
          defaultMode: 110
      - name: inline-config-sources
        secret:
          secretName: release-name-gitea-inline-config
      - name: temp
        emptyDir: {}
      automountServiceAccountToken: false
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes:
      - ReadWriteOnce
      resources: {}
---
apiVersion: v1
kind: Pod
metadata:
  name: release-name-gitea-test-connection
  labels:
    helm.sh/chart: gitea-8.3.0
    app: gitea
    app.kubernetes.io/name: gitea
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: 1.19.3
    version: 1.19.3
    app.kubernetes.io/managed-by: Helm
  annotations:
    helm.sh/hook: test-success
    seccomp.security.alpha.kubernetes.io/pod: runtime/default
    container.apparmor.security.beta.kubernetes.io/wget: runtime/default
  namespace: test-ns
spec:
  containers:
  - name: wget
    image: busybox:1-uclibc@sha256:3dcd6bcb75d6ca5b23642482809ea3dbba4de07d277326454f443b9dcb14b012
    imagePullPolicy: IfNotPresent
    command:
    - wget
    args:
    - release-name-gitea-http:3000
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 25000
      capabilities:
        drop:
        - ALL
      runAsNonRoot: true
      runAsGroup: 25000
      readOnlyRootFilesystem: true
    resources:
      limits:
        cpu: 250m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 128Mi
    readinessProbe:
      exec:
        command:
        - ls
        - /
      initialDelaySeconds: 30
      periodSeconds: 10
    livenessProbe:
      exec:
        command:
        - ls
        - /
      initialDelaySeconds: 30
      periodSeconds: 10
  restartPolicy: Never
  securityContext:
    runAsUser: 25000
    seccompProfile:
      type: RuntimeDefault
    runAsNonRoot: true
    runAsGroup: 25000
  serviceAccountName: SAtest0
  automountServiceAccountToken: false
---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
  namespace: test-ns
spec:
  limits:
  - max:
      cpu: 800m
    min:
      cpu: 250m
    type: Container
---
apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-min-max-demo-lr
  namespace: default
spec:
  limits:
  - max:
      cpu: 800m
    min:
      cpu: 250m
    type: Container
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pods-high
  namespace: test-ns
spec:
  hard:
    cpu: '1000'
    memory: 200Gi
    pods: '10'
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values:
      - high
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: pods-high
  namespace: default
spec:
  hard:
    cpu: '1000'
    memory: 200Gi
    pods: '10'
  scopeSelector:
    matchExpressions:
    - operator: In
      scopeName: PriorityClass
      values:
      - high
